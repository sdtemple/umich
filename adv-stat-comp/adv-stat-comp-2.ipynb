{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e004a65",
   "metadata": {},
   "source": [
    "### 1: STRUCTURE\n",
    "\n",
    "In human genetics, a common task is to collect [genotype data](https://en.wikipedia.org/wiki/Genotype#Genotyping) from a collection of individuals, and study this data in order to understand what population(s) the individuals belong to. The program [STRUCTURE](https://academic.oup.com/genetics/article/155/2/945/6048111) (Pritchard, Stephens & Donnelly, 2003) is an [influential method](https://academic.oup.com/genetics/article/204/2/391/6072054) for doing this. \n",
    "\n",
    "Formally, given a matrix $G \\in \\{0,1\\}^{N \\times L \\times 2}$ of diploid genotype data from $N$ individuals type at $L$ loci, and some estimate $K$ of the number of populations, STRUCTURE assumes that $G$ is generated according to the following probabilistic model:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_{n\\ell a} & \\sim \\mathrm{Binom}(1,P_{Z_{n\\ell}\\ell})\\\\\n",
    "P_{k \\ell} & \\sim \\mathrm{Beta}(fp,f(1-p)) \\\\\n",
    "Z_{n\\ell a} &\\sim \\mathrm{Categorical}(Q_n) \\\\\n",
    "Q_n &\\sim \\mathrm{Dirichlet}(\\alpha) \\in \\Delta^{K-1}\n",
    "\\end{align} \n",
    "$$\n",
    "Here $F,p\\in[0,1]$ and $\\alpha \\in \\mathbb{R}_{+}^{K}$ are hyperparameters.\n",
    "\n",
    "This model is closely related to the latent Dirichlet allocation method of for document topic modeling. \n",
    "\n",
    "Here is a pure Python implementation of a Gibbs sampler for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gibbs_sampler(G, K, rng, num_iterations=1000, alpha=1.0, F=.1, p=.1):\n",
    "    \"\"\"\n",
    "    Run Gibbs sampler for the STRUCTURE model.\n",
    "    \n",
    "    data: N x L x 2 array of genotype data\n",
    "    K: Number of populations\n",
    "    rng: Random number generator.\n",
    "    num_iterations: Number of Gibbs sampling iterations\n",
    "    alpha: Dirichlet prior parameter for q\n",
    "    F: estimate of Fst between populations.\n",
    "    p: mean allele frequency\n",
    "    \"\"\"\n",
    "    N, L, _ = G.shape\n",
    "    assert G.shape == (N, L, 2)\n",
    "    # Initialize parameters\n",
    "    Q = rng.dirichlet([alpha] * K, size=N)\n",
    "    f = (1 - F) / F\n",
    "    f, p = [np.broadcast_to(x, [K, L]) for x in [f, p]] \n",
    "    a_prior = f * p\n",
    "    b_prior = f * (1 - p)\n",
    "    P = rng.beta(a_prior, b_prior)   # balding-nichols model\n",
    "    P = np.stack([P, 1 - P], axis=2)\n",
    "    # Initialize z assignments randomly\n",
    "    Z = rng.integers(0, K, size=(N, L, 2))\n",
    "    for iteration in range(num_iterations):\n",
    "        # Update z\n",
    "        for i in range(N):\n",
    "            for l in range(L):\n",
    "                for j in range(2):\n",
    "                    probs = Q[i] * P[:, l, G[i, l, j]]\n",
    "                    probs /= probs.sum()\n",
    "                    Z[i, l, j] = rng.choice(K, p=probs)\n",
    "        # Update p\n",
    "        for k in range(K):\n",
    "            for l in range(L):\n",
    "                counts = np.zeros([2])\n",
    "                idx = np.where(Z[:,l,:] == k)\n",
    "                alleles = G[:,l,:][idx]\n",
    "                for a in range(2):\n",
    "                    counts[a] = np.sum(alleles == a)\n",
    "                P[k, l] = rng.beta(counts[0] + a_prior[k, l], counts[1] + b_prior[k, l])\n",
    "        # Update q\n",
    "        for i in range(N):\n",
    "            counts = np.zeros(K)\n",
    "            for k in range(K):\n",
    "                counts[k] = np.sum(Z[i,:,:] == k)\n",
    "            Q[i] = rng.dirichlet(counts + alpha)\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}\")\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad72e0e",
   "metadata": {},
   "source": [
    "1. Rewrite the above sampler using any of the techniques we have learned in class so far (vectorization, Numba, Jax, Cuda/GPU, Cython, multithreading/multiprocessing) in order to get a speedup.\n",
    "2. Report the speedup you obtained using benchmarking.\n",
    "3. Explain why your code is faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ddebd",
   "metadata": {},
   "source": [
    "Test your implementation using the following code, which simulates a model with three populations and returns a genotype matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33790fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stdpopsim\n",
    "species = stdpopsim.get_species(\"HomSap\")\n",
    "model = species.get_demographic_model(\"OutOfAfrica_3G09\")\n",
    "chrom = species.get_contig('1', length_multiplier=0.01)\n",
    "samples = model.get_samples(10, 10, 10)\n",
    "engine = stdpopsim.get_engine(\"msprime\")\n",
    "ts = engine.simulate(model, chrom, samples)\n",
    "\n",
    "G = ts.genotype_matrix().T\n",
    "# convert diploid to tensor\n",
    "G = np.array([[0, 0], [0, 1], [1, 1]])[G]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "p = G.mean((0, 2))\n",
    "F = ts.Fst([np.arange(10), np.arange(10, 20)])\n",
    "gibbs_sampler(G, 3, rng, p=p, F=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadf3b3",
   "metadata": {},
   "source": [
    "### 2. Composite likelihood\n",
    "\n",
    "[Composite likelihood](https://www3.stat.sinica.edu.tw/sstest/oldpdf/A21n11.pdf) is a technique for performing approximate likelihood-based inference in models where the full likelihood is intractable. The basic idea is replace the overall likelihood with a \"product of marginals\" which can be tractably computed. \n",
    "\n",
    "For example, consider the spatial 2D Gaussian process model \n",
    "\n",
    "$$y(s) = \\mu + \\varepsilon(s),$$\n",
    "\n",
    "where $s=(x,y)$, $y(s)$ is the observed value at location $s$, and the error term is zero-mean Gaussian process with covariance function \n",
    "\n",
    "$$\\mathrm{cov}(\\varepsilon(s_i), \\varepsilon(s_j)) = \\sigma^2 \\exp \\left(-\\frac{\\| s_i - s_j \\|}{\\phi} \\right)$$\n",
    "\n",
    "for variance parameter $\\sigma^2>0$ and range parameter $\\phi>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28d4b2",
   "metadata": {},
   "source": [
    "\n",
    "1. Suppose you are given a $10^8$-dimensional sample $\\mathbf{y}=(y(s_1),\\dots,y(s_{10^7}))$ from the GP.\n",
    "   \n",
    "   a. Explain why estimating $\\mu$ is easy. (Explain why we can assume $\\mu=0$.)\n",
    "   \n",
    "   b. Explain why estimating $\\sigma^2$ and $\\phi$ using e.g. naive maximum likelihood is challenging.\n",
    "\n",
    "2. Consider the composite likelihood function \n",
    "\n",
    "    $$\\ell_c(\\mathbf{y} \\mid \\theta) = \\sum_{i,j \\in S} \\log \\mathbb{P}(y(s_{i}), y(s_{j}) \\mid \\theta),$$\n",
    "\n",
    "    where $\\theta=(\\sigma^2,\\phi)$, and $S\\subset [n]^2$ is some subset of indices. Here $\\mathbb{P}(y(s_{i}), y(s_{j}) \\mid \\theta)$ denotes the marginal distribution of $y$ at the two points $s_i$ and $s_i$. By the definition of GP, it is bivariate normal with covariance shown above.\n",
    "\n",
    "    a. Explain why computing $\\ell_c$ is easier (especially if you are smart about choosing $S$.)\n",
    "\n",
    "    b. Argue that maximizing $\\ell_c$ over $\\theta$ results in an unbiased estimator.\n",
    "\n",
    "    c. Explain intuitively (no math needed) why applying standard asymptotic theory to the resulting MCLE (maximum *composite* likelihood estimator) would result in the wrong covariance matrix. Would confidence intervals tend to be too narrow or too broad? Why? (Hint: consider the \"effective sample size\".)\n",
    "    \n",
    "3. The correct asymptotic covariance matrix for the MCLE is called the Godambe Information Matrix. It is given by\n",
    "\n",
    "    $$G = H^{-1} J H^{-1}$$\n",
    "\n",
    "    where $H = -\\nabla^2_\\theta \\ell_c(\\hat{\\theta})$ is the observed information, $U_c(\\theta)=\\nabla_\\theta \\ell_c(\\theta)$ is the score function, and $J$ is an estimate (sample covariance) of $\\mathrm{cov}\\, U_c(\\hat{\\theta}).$ \n",
    "    \n",
    "    (Notice that $J=H$ for the usual likelihood function -- this is called [Bartlett's second identity](https://math.stackexchange.com/questions/2026428/what-is-second-bartlett-identity) and is the reason why the Fisher information $F=J=H$.)\n",
    "\n",
    "    a. Perform a small simulation study which estimates the covariance of $\\hat{\\theta}$ when $\\theta$ maximizes a composite likelihood.\n",
    "\n",
    "    b. Use automatic differentiation to compute $G$ for your choice of composite likelihood function, as well as $F$, the observed Fisher information.\n",
    "\n",
    "    c. Compare the results of part a) with $F$ and $G$. Does $F$ under/overestimate asymptotic variance and if so by how much?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
